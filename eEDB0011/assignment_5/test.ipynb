{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkContext\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_VERSION\"]='3.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.analyzers import AnalysisRunner\n",
    "\n",
    "from pydeequ.analyzers import AnalyzerContext\n",
    "from pydeequ import deequ_maven_coord\n",
    "from pydeequ import f2j_maven_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/28 21:26:36 WARN Utils: Your hostname, kvothe resolves to a loopback address: 127.0.1.1; using 192.168.100.21 instead (on interface wlp2s0)\n",
      "23/08/28 21:26:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/senoj/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/senoj/.ivy2/cache\n",
      "The jars for the packages stored in: /home/senoj/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c3c70c8e-e1cd-43a3-8d73-66c5389f62a4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;2.0.3-spark-3.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "downloading https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.3-spark-3.3/deequ-2.0.3-spark-3.3.jar ...\n",
      "\t[SUCCESSFUL ] com.amazon.deequ#deequ;2.0.3-spark-3.3!deequ.jar (1673ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.10!scala-reflect.jar (2194ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze_2.12/0.13.2/breeze_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze_2.12;0.13.2!breeze_2.12.jar (5055ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze-macros_2.12/0.13.2/breeze-macros_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze-macros_2.12;0.13.2!breeze-macros_2.12.jar (366ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...\n",
      "\t[SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (418ms)\n",
      "downloading https://repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar ...\n",
      "\t[SUCCESSFUL ] net.sf.opencsv#opencsv;2.3!opencsv.jar (383ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.rwl#jtransforms;2.4.0!jtransforms.jar (582ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.2!commons-math3.jar (2053ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire_2.12/0.13.0/spire_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire_2.12;0.13.0!spire_2.12.jar (4435ms)\n",
      "downloading https://repo1.maven.org/maven2/com/chuusai/shapeless_2.12/2.3.2/shapeless_2.12-2.3.2.jar ...\n",
      "\t[SUCCESSFUL ] com.chuusai#shapeless_2.12;2.3.2!shapeless_2.12.jar(bundle) (1930ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.5!slf4j-api.jar (302ms)\n",
      "downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] junit#junit;4.8.2!junit.jar (433ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire-macros_2.12/0.13.0/spire-macros_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire-macros_2.12;0.13.0!spire-macros_2.12.jar (396ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/machinist_2.12/0.6.1/machinist_2.12-0.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#machinist_2.12;0.6.1!machinist_2.12.jar (402ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/macro-compat_2.12/1.1.1/macro-compat_2.12-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#macro-compat_2.12;1.1.1!macro-compat_2.12.jar (288ms)\n",
      ":: resolution report :: resolve 14038ms :: artifacts dl 20935ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.3-spark-3.3 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.1 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   15  |   15  |   2   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c3c70c8e-e1cd-43a3-8d73-66c5389f62a4\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (33137kB/108ms)\n",
      "23/08/28 21:27:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# create spark session and spark context\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", f2j_maven_coord)\n",
    "    .getOrCreate())\n",
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Segmento: string, CNPJ: string, Nome: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = spark.read.csv(\"./app/data/Bancos\", header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+------------------+\n",
      "| entity|instance|         name|             value|\n",
      "+-------+--------+-------------+------------------+\n",
      "| Column|    CNPJ|CountDistinct|            1459.0|\n",
      "|Dataset|       *|         Size|            1474.0|\n",
      "| Column|Segmento| Completeness|               1.0|\n",
      "| Column|    CNPJ| Distinctness|0.9898236092265943|\n",
      "| Column|    CNPJ| Completeness|               1.0|\n",
      "+-------+--------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.analyzers import Size\n",
    "from pydeequ.analyzers import Completeness\n",
    "from pydeequ.analyzers import CountDistinct\n",
    "from pydeequ.analyzers import Distinctness\n",
    "\n",
    "analysis_result = AnalysisRunner(spark).onData(df)\\\n",
    "                                            .addAnalyzer(Size())\\\n",
    "                                            .addAnalyzer(Completeness(\"Segmento\"))\\\n",
    "                                            .addAnalyzer(Completeness(\"CNPJ\"))\\\n",
    "                                            .addAnalyzer(CountDistinct(\"CNPJ\"))\\\n",
    "                                            .addAnalyzer(Distinctness(\"CNPJ\"))\\\n",
    "                                            .run()\n",
    "analysis_result_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysis_result)\n",
    "analysis_result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|CompletenessConst...|          Failure|Value: 1.0 does n...|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.checks import Check\n",
    "from pydeequ.checks import CheckLevel\n",
    "from pydeequ.verification import VerificationSuite\n",
    "from pydeequ.verification import VerificationResult\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.isContainedIn(\"Segmento\", [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"])\\\n",
    "             .hasCompleteness(\"CNPJ\", lambda x: x >= 0.98))\\\n",
    "    .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
